
# ğŸ§  Dual Response Comparison Engine (DRCE)

Toolset for evaluating how closely a sideloaded LLM with a *mindfile* replicates the personality and answers of a reference human.

Designed to support both **manual input** (e.g., via ChatGPT interface) and **automated testing pipelines**, with two core comparison modes: **LLM-based similarity scoring** and **Arena-style Elo ranking**.
Please, read the abstract.md file.

---

## ğŸ“Œ Use Cases

- Evaluate alignment between a digital personality and its original.
- Track quality evolution across versions of a mindfile.
- Use human feedback or LLM judgment to score fidelity.
- Establish a repeatable benchmark for digital twin quality.

---

## ğŸ§± File Structure

```

drce/
â”œâ”€â”€ responses/
â”‚   â”œâ”€â”€ roman.md               # Human answers
â”‚   â”œâ”€â”€ roman\_v1\_gpt4.md       # Mindfile v1 (GPT-4)
â”‚   â””â”€â”€ roman\_v2\_claude.md     # Mindfile v2 (Claude)
â”œâ”€â”€ questions.txt              # Input question list
â”œâ”€â”€ comparison.md              # Final Markdown output
â”œâ”€â”€ comparison\_scores.json     # LLM-based scoring output
â”œâ”€â”€ elo\_scores.json            # Arena-based scoring output
â”œâ”€â”€ compare\_with\_llm.py        # Script for LLM comparisons
â”œâ”€â”€ arena\_gui.py               # GUI for Elo comparisons
â”œâ”€â”€ parse\_markdown.py          # Markdown parser utility
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Project documentation

````

---

## âš™ï¸ Installation

```bash
git clone https://github.com/Sideloading-Research/Dual-Response-Comparison-Engine-DRCE-.git
cd Dual-Response-Comparison-Engine-DRCE-
pip install -r requirements.txt
````

---

## ğŸ“ 1. Input Preparation

### `questions.txt`

Plain text list of questions, one per line:

```
What is your name?
Do you like pizza?
...
```

### `responses/*.md`

Markdown-style answer files:

```markdown
# What is your name?
Roman

# Do you like pizza?
Yes
```

Use one file per version (e.g., `roman.md`, `roman_v1_gpt4.md`, etc).

---

## ğŸš€ 2. Run Comparisons

### ğŸ§  A. Automatic LLM Comparison

```bash
python compare_with_llm.py \
  --a responses/roman.md \
  --b responses/roman_v1_gpt4.md \
  --model gpt-4 \
  --output comparison_scores.json
```

* Uses an external LLM to score each Q-A pair by:

  * Semantic similarity
  * Stylistic fidelity
  * Human-likeness
* Outputs scores and a `comparison.md` file.

---

### ğŸ§‘â€âš–ï¸ B. Arena-Style Elo Comparison (Manual GUI)

```bash
python arena_gui.py \
  --a responses/roman.md \
  --b responses/roman_v1_gpt4.md
```

* Presents Q and two anonymized answers (A vs. B).
* User selects the more "authentic" one.
* Elo score is updated accordingly.
* Results saved to `elo_scores.json`.

---

## ğŸ“„ 3. Output Artifacts

### `comparison.md`

```markdown
## What is your name?

- ğŸ§ Original: Roman  
- ğŸ¤– Mindfile-v1-GPT4: My name is Roman.

LLM Similarity Score: 9.5
```

### `comparison_scores.json`

```json
{
  "What is your name?": {
    "original": "Roman",
    "mindfile": "My name is Roman.",
    "similarity": 9.5
  },
  ...
}
```

### `elo_scores.json`

```json
{
  "roman.md": 1540,
  "roman_v1_gpt4.md": 1460
}
```

---

## ğŸ”§ Configuration Options

Both comparison scripts accept:

* `--a`, `--b`: Markdown files to compare.
* `--model`: LLM to use (`gpt-4`, `claude-3-opus`, etc.).
* `--output`: JSON output path.
* `--system_prompt`: (Optional) Context to inject before comparison.

---

## ğŸ”¬ Evaluation Strategies

| Mode           | Requires LLM | Human Judgment | Quantitative |
| -------------- | ------------ | -------------- | ------------ |
| LLM Similarity | âœ…            | âŒ              | âœ…            |
| Arena Elo      | âŒ            | âœ…              | âœ…            |

| Method         | Output                   | Manual | Objective |
| -------------- | ------------------------ | ------ | --------- |
| LLM similarity | `comparison_scores.json` | âŒ      | âœ…         |
| Arena GUI      | `elo_scores.json`        | âœ…      | âœ…         |


Both modes are complementary.

## âœ… Summary of Supported Workflows
| Task                              | Manual | Automated |
| --------------------------------- | ------ | --------- |
| Human answer collection           | âœ…      | âŒ         |
| Sideloaded LLM answer collection  | âœ…      | âœ…         |
| Answer comparison via LLM         | âŒ      | âœ…         |
| Answer comparison via Arena (GUI) | âœ…      | âœ…         |
| Markdown report generation        | âœ…      | âœ…         |

---

## ğŸ› ï¸ Future Work

* Web dashboard for batch comparisons.
* Support for sentence-transformer metrics (BERTScore, cosine).
* CSV export of Q-A-Similarity tables.
* API endpoint for continuous evaluation.
* Friend Peer Review (FPR) integration.

---

## ğŸ“œ License

MIT License (customize as needed)

---

## ğŸ™‹ Contribution

Feel free to fork, adapt, and improve. Open issues or PRs are welcome.


